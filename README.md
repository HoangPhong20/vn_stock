# VN_stock â€“ Vietnam Stock Data Pipeline

## ðŸŽ¯ Project Overview

End-to-end **Data Engineering pipeline** cho dá»¯ liá»‡u chá»©ng khoÃ¡n Viá»‡t Nam, Ã¡p dá»¥ng mÃ´ hÃ¬nh **Bronze â†’ Silver â†’ Gold**, dÃ¹ng **Python + DuckDB** lÃ m analytical warehouse.

Má»¥c tiÃªu:

* XÃ¢y dá»±ng pipeline rÃµ rÃ ng, tÃ¡ch extract / transform / load
* Thá»±c hÃ nh data modeling (fact / dimension)
* Viáº¿t analytics & BI queries trá»±c tiáº¿p trÃªn DuckDB

PhÃ¹ há»£p cho **Data Engineer / Analytics Engineer portfolio**.

---

## ðŸ—ï¸ Project Structure

```text
VN_stock/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                  # Global configs
â”‚
â”œâ”€â”€ duckdb/
â”‚   â””â”€â”€ vnstock.duckdb               # DuckDB warehouse (local, demo)
â”‚
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ stock_price_silver.yaml      # Silver schema definition
â”‚   â””â”€â”€ stock_price_gold.yaml        # Gold schema definition
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py              # Pipeline entrypoint
â”‚   â””â”€â”€ duckdb_analysis.py           # Analytics & BI queries
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â””â”€â”€ fetch_stock_price.py     # Ingest raw stock data
â”‚   â”‚
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”œâ”€â”€ clean_stock_price.py     # Clean & standardize (Silver)
â”‚   â”‚   â”œâ”€â”€ build_stock_price_daily.py # Build fact table (Gold)
â”‚   â”‚   â”œâ”€â”€ validate_silver_schema.py
â”‚   â”‚   â””â”€â”€ validate_gold_schema.py
â”‚   â”‚
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”œâ”€â”€ save_raw_to_s3.py         # Bronze
â”‚   â”‚   â”œâ”€â”€ save_silver_to_s3.py      # Silver
â”‚   â”‚   â””â”€â”€ save_gold_to_s3.py        # Gold
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ s3_utils.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ðŸ§± Data Modeling

### Fact tables

* **fact_stock_price** â€“ daily grain `(trading_date, symbol)`
* **fact_stock_price_monthly** â€“ `(year_month, symbol)`
* **fact_stock_price_yearly** â€“ `(year, symbol)`

### Dimension tables

* **dim_date**
* **dim_symbol**

---

## ðŸ”„ Pipeline Flow

```text
Raw API
  â†“
Bronze (raw parquet on S3)
  â†“
Silver (cleaned, validated)
  â†“
Gold (fact / dim tables)
  â†“
DuckDB analytics & BI queries
```

---

## ðŸš€ How to Run

### 1ï¸âƒ£ Environment setup

```bash
conda create -n spark python=3.10
conda activate spark
pip install -r requirements.txt
```

Create `.env`:

```env
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
AWS_DEFAULT_REGION=your_region
```

---

### 2ï¸âƒ£ Run full pipeline

```bash
python -m scripts.run_pipeline
```

---

### 3ï¸âƒ£ Run analytics & checks

```bash
python -m scripts.duckdb_analysis
```

Example output:

```text
DuckDB ready ðŸš€
fact_stock_price rows: 158124
fact_stock_price orphan date_key: 0
Top volume by exchange/year:
(2025, 'HOSE', 226540745985)
```

---

## ðŸ“Š Example BI Query

```sql
SELECT
  symbol,
  ROUND(AVG(close), 2) AS avg_close_30d
FROM v_gold_stock_price_daily
WHERE trading_date >= CURRENT_DATE - INTERVAL 30 DAY
GROUP BY symbol
ORDER BY avg_close_30d DESC
LIMIT 10;
```

---

## âœ… Data Quality Checks

* Duplicate grain validation
* Null surrogate keys
* Orphan foreign keys
* Schema validation via YAML

---

## ðŸ§  Tech Stack

* Python
* DuckDB
* SQL Analytics
* Parquet
* AWS S3 (optional)

---

